{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cv2 import *\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_data(movie_path, frame_indices, headless=False, background_subtract=False, randomize=False):\n",
    "    data = []\n",
    "    start_frame, end_frame = frame_indices\n",
    "    cap = cv2.VideoCapture(movie_path)\n",
    "    i = 1\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        i += 1\n",
    "        if start_frame < i < end_frame:\n",
    "            if randomize:\n",
    "                pass\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            if background_subtract:\n",
    "                frame = cv2.subtract(frame, background)\n",
    "            \n",
    "            if not headless:\n",
    "                cv2.imshow('Frame', frame)\n",
    "                time.sleep(0.05)\n",
    "            \n",
    "            data.append(frame)\n",
    "\n",
    "        if i > end_frame or cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return data, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(data, verbose=False):    \n",
    "    fig, axs = plt.subplots(2, 5, figsize=(15, 15))\n",
    "    indices = iter(np.linspace(0, len(data)-1, 10))\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            index = int(next(indices))\n",
    "            frame = data[index]\n",
    "            axs[i][j].xaxis.set_visible(False)\n",
    "            axs[i][j].yaxis.set_visible(False)\n",
    "            axs[i][j].imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample(subject, scene, gesture, headless=False):    \n",
    "    # Get labels' path    \n",
    "    subject_path_for_labels = 'subject0{}'.format(subject) if (subject < 10) else 'subject{}'.format(subject)\n",
    "    scene_path_for_labels = 'Scene{}'.format(scene)\n",
    "    labels_path = 'labels/{}/{}'.format(subject_path_for_labels, scene_path_for_labels)\n",
    "    \n",
    "    # Change directory to current directory.\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(labels_path)\n",
    "    groups = os.listdir(os.getcwd())\n",
    "    \n",
    "    # Iterate through all gesture-frame pairs\n",
    "    for group_csv, group_num in zip(groups, range(1, len(groups) + 1)):\n",
    "        frames = pd.read_csv(group_csv, header=None)\n",
    "        if gesture in frames[0].values:\n",
    "            os.chdir(cwd)\n",
    "            frame_indices = frames[frames[0] == gesture].values[0][1:]\n",
    "            \n",
    "            # Get video path\n",
    "            subject_path_for_videos = 'Subject0{}'.format(subject) if subject < 10 else 'Subject{}'.format(subject)\n",
    "            scene_path_for_videos = 'Scene{}'.format(scene)\n",
    "            video_path = 'Videos/{}/{}/Color/rgb{}.avi'.format(subject_path_for_videos, scene_path_for_videos, group_num)\n",
    "        \n",
    "            # Load gesture data\n",
    "            data = load_video_data(video_path, frame_indices, headless)\n",
    "            return data\n",
    "        \n",
    "    os.chdir(cwd)    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_43 = []\n",
    "data_14 = []\n",
    "\n",
    "for subject in range(1, 31):\n",
    "    data_43.append(load_sample(subject, 1, 43, headless=True))\n",
    "    data_14.append(load_sample(subject, 1, 14, headless=True))\n",
    "\n",
    "data_43 = [(x[0], [1, 0]) for x in data_43]\n",
    "data_14 = [(x[0], [0, 1]) for x in data_14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def star_clip(clip, euclidean=False):\n",
    "    \n",
    "    N = len(clip)\n",
    "    M = np.zeros(clip[0].shape)\n",
    "    for k in range(2, N):\n",
    "        w_s = k/N\n",
    "        I_k_1 = clip[k-1]\n",
    "        I_k = clip[k]\n",
    "        if euclidean:\n",
    "            frame_diff = (I_k_1 - I_k) \n",
    "        frame_diff = I_k_1 - I_k\n",
    "        delta = np.abs(frame_diff) * w_s\n",
    "        M += delta\n",
    "    \n",
    "    M_x = cv2.Sobel(M, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    M_y = cv2.Sobel(M, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    star = np.array([M, M_x, M_y])\n",
    "    \n",
    "    return star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_43 = [(star_clip(sample[0]), sample[1]) for sample in data_43]\n",
    "star_14 = [(star_clip(sample[0]), sample[1]) for sample in data_14]\n",
    "\n",
    "star_samples = star_43 + star_14\n",
    "np.random.shuffle(star_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(data, batch_size, shuffle=False):    \n",
    "    batches, labels = [], []\n",
    "    i = 0\n",
    "    while i < len(star_samples):\n",
    "        batches.append(torch.tensor([sample[0] for sample in star_samples[i:i+4]]).float())\n",
    "        labels.append(torch.tensor([sample[1] for sample in star_samples[i:i+4]]).long())\n",
    "        i += batch_size\n",
    "    return batches, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c71ea604943f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstar_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-392cfc3388c9>\u001b[0m in \u001b[0;36mprepare_batches\u001b[1;34m(data, batch_size, shuffle)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstar_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mbatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstar_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstar_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "data, labels = prepare_batches(star_samples, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(16 * 117 * 157, 500)\n",
    "        \n",
    "#     def encode(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, 16 * 117 * 157)\n",
    "#         x = self.fc1(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Reshape, Input, InputLayer\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "def build_autoencoder(img_shape, code_size):\n",
    "    # The encoder\n",
    "    encoder = Sequential()\n",
    "    encoder.add(InputLayer(img_shape))\n",
    "    encoder.add(Flatten())\n",
    "    encoder.add(Dense(code_size))\n",
    "\n",
    "    # The decoder\n",
    "    decoder = Sequential()\n",
    "    decoder.add(InputLayer((code_size,)))\n",
    "    decoder.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "    decoder.add(Reshape(img_shape))\n",
    "\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder = build_autoencoder((3, 480, 640), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 480, 640)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_samples[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
